# Text Predictor

## ðŸ“Œ Project Overview
Text Predictor is a mini-project that utilizes a Long Short-Term Memory (LSTM) model to predict the next word in a sequence. The model is trained on a small dataset and demonstrates how deep learning can be applied to natural language processing (NLP) tasks.

## ðŸ“‚ Dataset
- The model is trained on a **small dataset** containing text sequences.
- Preprocessing includes tokenization, padding, and text encoding.

## ðŸ”§ Technologies Used
- **Python**
- **TensorFlow/Keras** (for LSTM implementation)
- **NumPy** (for numerical operations)
- **Pandas** (for data manipulation)
- **NLTK / SpaCy** (for text preprocessing)

## ðŸš€ Model Architecture
The model consists of the following layers:
1. **Embedding Layer** â€“ Converts words into dense vector representations.
2. **LSTM Layer** â€“ Captures sequential dependencies in text.
3. **Dense Layer** â€“ Outputs probabilities for the next word prediction.





